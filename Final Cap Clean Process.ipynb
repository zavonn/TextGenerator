{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\zvnow\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#!pip install newspaper3k\n",
    "#!pip install newsapi-python\n",
    "#!pip install langid\n",
    "#!pip install ipython-autotime\n",
    "\n",
    "%load_ext autotime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from newsapi import NewsApiClient\n",
    "import pickle\n",
    "import nltk\n",
    "nltk.download('words')\n",
    "from nltk.corpus import words\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import newspaper\n",
    "from newspaper import article\n",
    "from urllib.parse import urlsplit\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import umap\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from random import randrange\n",
    "import langid\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://feedproxy.google.com/',\n",
       " 'http://stream.aljazeera.com/',\n",
       " 'https://www.cbsnews.com/',\n",
       " 'https://thenextweb.com/',\n",
       " 'https://www.polygon.com/',\n",
       " 'https://www.irishtimes.com/',\n",
       " 'https://markets.businessinsider.com/',\n",
       " 'https://www.medicalnewstoday.com/',\n",
       " 'https://play.google.com/',\n",
       " 'https://www.businessinsider.com/',\n",
       " 'https://in.reuters.com/',\n",
       " 'https://m.news24.com/',\n",
       " 'https://mmajunkie.usatoday.com/',\n",
       " 'https://www.ccn.com/',\n",
       " 'https://www.nationalgeographic.com/',\n",
       " 'https://www.jpost.com/',\n",
       " 'https://www.espncricinfo.com/',\n",
       " 'http://www.espncricinfo.com/',\n",
       " 'https://nymag.com/',\n",
       " 'https://www.msnbc.com/',\n",
       " 'https://www.aljazeera.com/',\n",
       " 'https://www.indiatimes.com/',\n",
       " 'https://edition.cnn.com/',\n",
       " 'http://www.msnbc.com/',\n",
       " 'https://ca.reuters.com/',\n",
       " 'https://ftw.usatoday.com/',\n",
       " 'https://ca.ign.com/',\n",
       " 'https://www.theverge.com/',\n",
       " 'https://thehill.com/',\n",
       " 'http://feeds.ign.com/',\n",
       " 'https://www.usatoday.com/',\n",
       " 'https://www.football-italia.net/',\n",
       " 'https://apnews.com/',\n",
       " 'https://www.cbc.ca/',\n",
       " 'https://arstechnica.com/',\n",
       " 'https://techcrunch.com/',\n",
       " 'https://www.cnn.com/',\n",
       " 'https://disneyparks.disney.go.com/',\n",
       " 'https://www.ign.com/',\n",
       " 'https://in.ign.com/',\n",
       " 'https://www.theamericanconservative.com/',\n",
       " 'http://www.mtv.com/',\n",
       " 'https://www.wsj.com/',\n",
       " 'https://www.nfl.com/',\n",
       " 'https://www.reuters.com/',\n",
       " 'https://in.mashable.com/',\n",
       " 'http://wpcomics.washingtonpost.com/',\n",
       " 'https://business.financialpost.com/',\n",
       " 'https://bleacherreport.com/',\n",
       " 'https://www.washingtonpost.com/',\n",
       " 'https://fortune.com/',\n",
       " 'https://www.nextbigfuture.com/',\n",
       " 'https://www.news.com.au/',\n",
       " 'https://www.fourfourtwo.com/',\n",
       " 'https://www.buzzfeed.com/',\n",
       " 'http://www.nfl.com/',\n",
       " 'https://www.foxnews.com/',\n",
       " 'https://www.thehindu.com/',\n",
       " 'http://video.foxnews.com/',\n",
       " 'https://www.newsweek.com/',\n",
       " 'https://time.com/',\n",
       " 'https://www.abc.net.au/',\n",
       " 'https://www.bbc.co.uk/',\n",
       " 'https://www.afr.com/',\n",
       " 'https://news.ycombinator.com/',\n",
       " 'https://new.reddit.com/',\n",
       " 'http://nymag.com/',\n",
       " 'https://jp.techcrunch.com/',\n",
       " 'https://www.breitbart.com/',\n",
       " 'https://blog.ycombinator.com/',\n",
       " 'https://colab.research.google.com/',\n",
       " 'https://timesofindia.indiatimes.com/',\n",
       " 'https://ew.com/',\n",
       " 'https://www.newscientist.com/',\n",
       " 'https://af.reuters.com/',\n",
       " 'https://radio.foxnews.com/',\n",
       " 'https://news.google.com/',\n",
       " 'https://uk.reuters.com/',\n",
       " 'http://techcrunch.com/',\n",
       " 'https://blogs.wsj.com/',\n",
       " 'https://www.politico.com/',\n",
       " 'http://www.mtv.co.uk/',\n",
       " 'http://feeds.cbsnews.com/',\n",
       " 'https://br.ign.com/',\n",
       " 'https://www.news24.com/',\n",
       " 'http://www.techradar.com/',\n",
       " 'https://www.foxsports.com/',\n",
       " 'https://www.theglobeandmail.com/',\n",
       " 'https://www.vice.com/',\n",
       " 'https://www.independent.co.uk/',\n",
       " 'https://i-d.vice.com/',\n",
       " 'https://www.engadget.com/',\n",
       " 'https://www.nationalreview.com/',\n",
       " 'https://www.axios.com/',\n",
       " 'https://mashable.com/',\n",
       " 'https://www.rte.ie/',\n",
       " 'https://www.nbcnews.com/',\n",
       " 'http://www.washingtonpost.com/',\n",
       " 'https://www.washingtontimes.com/',\n",
       " 'https://economictimes.indiatimes.com/',\n",
       " 'https://www.cnbc.com/',\n",
       " 'https://abcnews.go.com/',\n",
       " 'https://www.techradar.com/',\n",
       " 'https://www.nhl.com/',\n",
       " 'https://www.reddit.com/',\n",
       " 'https://www.wired.com/',\n",
       " 'https://www.bloomberg.com/']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collect list of english news sources\n",
    "\n",
    "newsapi = NewsApiClient(api_key='b53e9d3a18c2485a95e99ad93f2f9828')\n",
    "\n",
    "sources = newsapi.get_sources()\n",
    "\n",
    "sources_df = pd.DataFrame(sources['sources'])\n",
    "\n",
    "list_src = sources_df.loc[sources_df['language']=='en']['id'].values.tolist()\n",
    "\n",
    "all_articles = []\n",
    "\n",
    "for source in list_src: \n",
    "    all_articles1 = newsapi.get_everything(sources=source, language='en', page_size=100)\n",
    "    all_articles1 = all_articles1['articles']\n",
    "    all_articles += all_articles1\n",
    "    \n",
    "art_df = pd.DataFrame(all_articles)\n",
    "\n",
    "artik = art_df[['source','title','content']].dropna()\n",
    "\n",
    "urls_list1 = art_df['url'].values.tolist()\n",
    "\n",
    "base_urls1 = []\n",
    "for item in urls_list1:\n",
    "    split_url1 = urlsplit(item)\n",
    "    base_url1 = [split_url1.scheme + \"://\" + split_url1.netloc + \"/\"]\n",
    "    base_urls1 += base_url1\n",
    "    \n",
    "list(set(base_urls1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of Article Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually cut down list to sources of your choice\n",
    "\n",
    "source_list = ['https://ca.reuters.com/',\n",
    " 'https://www.cnbc.com/',\n",
    " 'https://fortune.com/',\n",
    " 'https://time.com/',\n",
    " 'https://uk.reuters.com/',\n",
    " 'https://www.aljazeera.com/',\n",
    " 'https://www.nationalreview.com/',\n",
    " 'https://www.independent.co.uk/',\n",
    " 'https://www.techradar.com/',\n",
    " 'https://www.news24.com/',\n",
    " 'https://www.wsj.com/',\n",
    " 'https://thehill.com/',\n",
    " 'http://techcrunch.com/',\n",
    " 'https://www.politico.com/',\n",
    " 'https://www.cbc.ca/',\n",
    " 'https://news.google.com/',\n",
    " 'https://www.bbc.co.uk/',\n",
    " 'https://www.engadget.com/',\n",
    " 'https://www.theverge.com/',\n",
    " 'https://blogs.wsj.com/',\n",
    " 'https://www.news.com.au/',\n",
    " 'https://nordic.ign.com/',\n",
    " 'https://city-press.news24.com/',\n",
    " 'https://www.cbsnews.com/',\n",
    " 'https://arstechnica.com/',\n",
    " 'http://www.washingtonpost.com/',\n",
    " 'https://news.ycombinator.com/',\n",
    " 'https://business.financialpost.com/',\n",
    " 'https://www.businessinsider.com/',\n",
    " 'https://abcnews.go.com/',\n",
    " 'https://www.jpost.com/',\n",
    " 'https://www.theamericanconservative.com/',\n",
    " 'https://www.bloomberg.com/',\n",
    " 'https://mashable.com/',\n",
    " 'https://www.reuters.com/',\n",
    " 'https://www.newscientist.com/',\n",
    " 'https://edition.cnn.com/',\n",
    " 'https://www.nationalgeographic.com/',\n",
    " 'https://www.nextbigfuture.com/',\n",
    " 'http://www.msnbc.com/',\n",
    " 'https://www.vice.com/',\n",
    " 'https://www.polygon.com/',\n",
    " 'http://nymag.com/',\n",
    " 'https://i-d.vice.com/',\n",
    " 'https://www.theglobeandmail.com/',\n",
    " 'https://www.axios.com/',\n",
    " 'https://www.washingtontimes.com/',\n",
    " 'https://apnews.com/',\n",
    " 'https://techcrunch.com/',\n",
    " 'https://www.newsweek.com/',\n",
    " 'https://www.buzzfeed.com/',\n",
    " 'https://markets.businessinsider.com/',\n",
    " 'https://www.breitbart.com/',\n",
    " 'https://www.ccn.com/',\n",
    " 'https://www.msnbc.com/',\n",
    " 'https://www.abc.net.au/',\n",
    " 'https://blog.ycombinator.com/',\n",
    " 'https://www.foxnews.com/',\n",
    " 'https://www.wired.com/',\n",
    " 'https://www.irishtimes.com/',\n",
    " 'https://www.cnn.com/',\n",
    " 'https://thenextweb.com/',\n",
    " 'https://www.ign.com/',\n",
    " 'https://www.usatoday.com/',\n",
    " 'https://www.washingtonpost.com/',\n",
    " 'https://www.nbcnews.com/']\n",
    "\n",
    "print(len(source_list))\n",
    "print(\"-------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use newspaper package to scrape articles from sites\n",
    "\n",
    "article_title = []\n",
    "article_content = []\n",
    "article_urls = []\n",
    "\n",
    "for url in source_list:\n",
    "    paper = newspaper.build(url, memoize_articles=False)\n",
    "    for article in paper.articles:\n",
    "        try:\n",
    "            article.download()\n",
    "            article.parse()\n",
    "        except:\n",
    "            pass\n",
    "        article_urls.append(article.url)\n",
    "        article_title.append(article.title)\n",
    "        article_content.append(article.text)\n",
    "\n",
    "print(\"Num of Article Titles: \", len(article_title))\n",
    "print(\"Num of Article Titles: \", len(article_content))\n",
    "print(\"Num of Article Titles: \", len(article_urls))\n",
    "\n",
    "article_src_url = []\n",
    "\n",
    "for item in article_urls:\n",
    "    split_url = urlsplit(item)\n",
    "    base_url = [split_url.netloc]\n",
    "    article_src_url += base_url\n",
    "\n",
    "article_df = pd.DataFrame(article_src_url, columns=['Source'])\n",
    "\n",
    "article_df['Title'] = article_title\n",
    "article_df['Content'] = article_content\n",
    "article_df['Article_url'] = article_urls\n",
    "\n",
    "article_df = article_df.dropna()\n",
    "article_df = article_df.reset_index(drop=True)\n",
    "article_df = article_df.iloc[article_df['Title'].drop_duplicates().index.tolist()] \n",
    "article_df = article_df.reset_index(drop=True)\n",
    "\n",
    "print(article_df.head())\n",
    "print(\"-------------------------------------------------------\")\n",
    "print(article_df.info())\n",
    "print(\"-------------------------------------------------------\")\n",
    "print(article_df['Source'].value_counts().describe())\n",
    "print(\"-------------------------------------------------------\")\n",
    "print(article_df['Source'].value_counts().head(50))\n",
    "print(\"-------------------------------------------------------\")\n",
    "\n",
    "article_df.to_pickle('article_df_1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 187 ms\n"
     ]
    }
   ],
   "source": [
    "#article_df = pd.read_pickle(\"article_df_1.pkl\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Length_content\n",
      "count     5063.000000\n",
      "mean       262.817697\n",
      "std        109.324801\n",
      "min        100.000000\n",
      "25%        172.000000\n",
      "50%        244.000000\n",
      "75%        345.000000\n",
      "max        500.000000\n",
      "-------------------------------------------------------\n",
      "           Source                                              Title  \\\n",
      "0  ca.reuters.com  Canada GDP edges up surprise 0.1% in November ...   \n",
      "1  ca.reuters.com  Prospects advance for Canada's oil pipelines, ...   \n",
      "2  ca.reuters.com  Palestinians face uphill battle against Trump'...   \n",
      "3  ca.reuters.com      TSX falls as coronavirus fears hit oil prices   \n",
      "4  ca.reuters.com  'I am not a virus': France's Asian community p...   \n",
      "\n",
      "                                             Content  \\\n",
      "0  ottawa reuters   the canadian economy grew by ...   \n",
      "1  winnipeg, manitoba reuters   prospects for exp...   \n",
      "2  jerusalem reuters   when palestinian leaders l...   \n",
      "3  reuters   canada’s main stock index fell on fr...   \n",
      "4  paris reuters   in a southeastern paris distri...   \n",
      "\n",
      "                                         Article_url  \\\n",
      "0  https://ca.reuters.com/article/topNews/idCAKBN...   \n",
      "1  https://ca.reuters.com/article/topNews/idCAKBN...   \n",
      "2  https://ca.reuters.com/article/topNews/idCAKBN...   \n",
      "3  https://ca.reuters.com/article/topNews/idCAKBN...   \n",
      "4  https://ca.reuters.com/article/topNews/idCAKBN...   \n",
      "\n",
      "                                            Title_cl  \\\n",
      "0  [canada, gdp, edges, surprise, november, cold,...   \n",
      "1  [prospects, advance, canada, oil, pipelines, h...   \n",
      "2  [palestinians, face, uphill, battle, trump, mi...   \n",
      "3  [tsx, falls, coronavirus, fears, hit, oil, pri...   \n",
      "4  [virus, france, asian, community, pushes, back...   \n",
      "\n",
      "                                           Title_str  \\\n",
      "0  canada gdp edges surprise november cold snap t...   \n",
      "1  prospects advance canada oil pipelines hurdles...   \n",
      "2  palestinians face uphill battle trump middle e...   \n",
      "3         tsx falls coronavirus fears hit oil prices   \n",
      "4  virus france asian community pushes back xenop...   \n",
      "\n",
      "                                          Content_cl  Length_content Cont_lang  \n",
      "0  [ottawa, reuters, the, canadian, economy, grew...             251        en  \n",
      "1  [winnipeg,, manitoba, reuters, prospects, for,...             259        en  \n",
      "2  [jerusalem, reuters, when, palestinian, leader...             382        en  \n",
      "3  [reuters, canada’s, main, stock, index, fell, ...             157        en  \n",
      "4  [paris, reuters, in, a, southeastern, paris, d...             291        en  \n",
      "-------------------------------------------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5063 entries, 0 to 5062\n",
      "Data columns (total 9 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   Source          5063 non-null   object\n",
      " 1   Title           5063 non-null   object\n",
      " 2   Content         5063 non-null   object\n",
      " 3   Article_url     5063 non-null   object\n",
      " 4   Title_cl        5063 non-null   object\n",
      " 5   Title_str       5063 non-null   object\n",
      " 6   Content_cl      5063 non-null   object\n",
      " 7   Length_content  5063 non-null   int64 \n",
      " 8   Cont_lang       5063 non-null   object\n",
      "dtypes: int64(1), object(8)\n",
      "memory usage: 356.1+ KB\n",
      "None\n",
      "-------------------------------------------------------\n",
      "time: 3min 11s\n"
     ]
    }
   ],
   "source": [
    "article_df_cl = article_df.copy()\n",
    "\n",
    "punct = ['!', \"''\", '#', '$', '%','&',\"'\",'(',')','*','+',',','-','.',',','/',':',';','<','=','>','?','@','[','\\\\',']','^','_','`','{','|','}','~']\n",
    "\n",
    "stop_words = list(set(stopwords.words('english')))\n",
    "stop_w = stop_words + punct\n",
    "\n",
    "article_df_cl = article_df_cl.apply(lambda x : x.str.replace('\\n', ' '))\n",
    "article_df_cl = article_df_cl.apply(lambda x : x.str.replace('  ', ' '))\n",
    "\n",
    "# Clean title for clustering\n",
    "\n",
    "article_df_cl['Title_cl'] = article_df_cl['Title'].str.lower()\n",
    "article_df_cl['Title_cl'] = article_df_cl['Title_cl'].apply(word_tokenize)\n",
    "\n",
    "for num in range(len(article_df_cl['Title_cl'])):\n",
    "    article_df_cl['Title_cl'][num] = [x for x in article_df_cl['Title_cl'][num] if x not in stop_w]\n",
    "    article_df_cl['Title_cl'][num] = [x for x in article_df_cl['Title_cl'][num] if len(x) > 2]\n",
    "    article_df_cl['Title_cl'][num] = [x for x in article_df_cl['Title_cl'][num] if x.isalpha()]\n",
    "    \n",
    "article_df_cl = article_df_cl[article_df_cl['Title_cl'].apply(lambda x: len(x) > 5)]\n",
    "article_df_cl['Title_str'] = article_df_cl['Title_cl'].apply(lambda x: \" \".join(x))\n",
    "\n",
    "article_df_cl = article_df_cl.reset_index(drop=True)\n",
    "\n",
    "# Clean content for filtering\n",
    "\n",
    "article_df_cl['Content_cl'] = article_df_cl['Content'].str.lower()\n",
    "article_df_cl['Content_cl'] = article_df_cl['Content_cl'].apply(word_tokenize)\n",
    "\n",
    "for num in range(len(article_df_cl['Content_cl'])):\n",
    "    article_df_cl['Content_cl'][num] = [x for x in article_df_cl['Content_cl'][num] if x not in stop_w]\n",
    "    article_df_cl['Content_cl'][num] = [x for x in article_df_cl['Content_cl'][num] if len(x) > 2]\n",
    "    article_df_cl['Content_cl'][num] = [x for x in article_df_cl['Content_cl'][num] if x.isalpha()]\n",
    "\n",
    "article_df_cl['Length_content'] = 0\n",
    "\n",
    "for num in range(len(article_df_cl['Content_cl'])):\n",
    "    article_df_cl['Length_content'][num] = len(article_df_cl['Content_cl'][num])\n",
    "    \n",
    "article_df_cl = article_df_cl[article_df_cl['Content_cl'].apply(lambda x: (len(x) > 99) & (len(x) < 501))]\n",
    "article_df_cl = article_df_cl.reset_index(drop=True)\n",
    "\n",
    "# Clean content\n",
    "\n",
    "article_df_cl['Content'] = article_df_cl['Content'].str.lower()\n",
    "article_df_cl['Content'] = article_df_cl['Content'].str.replace(\"-\",\" \").str.replace(\"(\",\"\").str.replace(\")\",\"\")\n",
    "article_df_cl['Content'] = article_df_cl['Content'].str.replace(\"“\",\"\").str.replace(\"”\",\"\").str.replace(\"]\",\"\").str.replace(\"[\",\"\").str.replace(\"\\\"\",\"\")\n",
    "\n",
    "# Filter non english content\n",
    "\n",
    "article_df_cl['Cont_lang'] = 0\n",
    "\n",
    "for num in range(len(article_df_cl['Content'])):\n",
    "    article_df_cl['Cont_lang'][num] = langid.classify(article_df_cl['Content'][num])[0]\n",
    "    \n",
    "article_df_cl = article_df_cl[article_df_cl['Cont_lang'] == 'en']\n",
    "\n",
    "# Reset Content_cl for text generation\n",
    "\n",
    "article_df_cl['Content_cl'] = article_df_cl['Content'].str.split()\n",
    "\n",
    "article_df_cl = article_df_cl.reset_index(drop=True)\n",
    "\n",
    "print(article_df_cl.describe())\n",
    "print(\"-------------------------------------------------------\")\n",
    "print(article_df_cl.head())\n",
    "print(\"-------------------------------------------------------\")\n",
    "print(article_df_cl.info())\n",
    "print(\"-------------------------------------------------------\")\n",
    "\n",
    "article_df_cl.to_pickle('article_df_cl_1.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5063, 10765)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 672 ms\n"
     ]
    }
   ],
   "source": [
    "#article_df_cl = pd.read_pickle('article_df_cl_1.pkl')\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "tfidf_fit = tfidf.fit(article_df_cl['Title_str'])\n",
    "\n",
    "tfidf_matrix = tfidf.transform(article_df_cl['Title_str'])\n",
    "\n",
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 93.7 ms\n"
     ]
    }
   ],
   "source": [
    "output = open('tfidf_matrix_art_1.pkl', 'wb')\n",
    "pickle.dump(tfidf_matrix, output)\n",
    "output.close()\n",
    "\n",
    "output = open('tfidf_mod_1.pkl', 'wb')\n",
    "pickle.dump(tfidf_fit, output)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_matrix = pickle.load(open(\"tfidf_matrix_art_1.pkl\", \"rb\"))\n",
    "# tfidf_fit = pickle.load(open(\"tfidf_mod_1.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UMAP on Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2min 21s\n"
     ]
    }
   ],
   "source": [
    "umap = umap.UMAP(n_neighbors=100, min_dist=0.1,\\\n",
    "                        n_components=50, random_state=42, metric='cosine')\n",
    "\n",
    "umap_result = umap.fit_transform(tfidf_matrix)\n",
    "\n",
    "umap_df = pd.DataFrame(umap_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 62.5 ms\n"
     ]
    }
   ],
   "source": [
    "umap_df.to_pickle('umap_df_1.pkl')\n",
    "pickle.dump(umap, open(\"umap_save_1.pkl\", \"wb\"))\n",
    "pickle.dump(umap_result, open(\"umap_result_1.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# umap = pickle.load(open(\"umap_save_1.pkl\", \"rb\"))\n",
    "# umap_result = pickle.load(open(\"umap_result_1.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means on UMAP Reduced Dataset (Umap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "time: 1.15 s\n"
     ]
    }
   ],
   "source": [
    "### Defining the k-means\n",
    "k1means_cluster = KMeans(n_clusters=10, random_state=42)\n",
    "\n",
    "### Fit model\n",
    "y1_pred = k1means_cluster.fit(umap_result)\n",
    "y1y_pred = y1_pred.predict(umap_result)\n",
    "\n",
    "print(\"------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 15.6 ms\n"
     ]
    }
   ],
   "source": [
    "pickle.dump(k1means_cluster, open(\"k1means_cluster_1.pkl\", \"wb\"))\n",
    "pickle.dump(y1_pred, open(\"y1_pred_1.pkl\", \"wb\"))\n",
    "pickle.dump(y1y_pred, open(\"y1y_pred_1.pkl\", \"wb\"))\n",
    "\n",
    "# k1means_cluster = pickle.load(open(\"k1means_cluster_1.pkl\", \"rb\"))\n",
    "# y1_pred = pickle.load(open(\"y1_pred_1.pkl\", \"rb\"))\n",
    "# y1y_pred = pickle.load(open(\"y1y_pred_1.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Cluster Number Assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count     10.000000\n",
      "mean     506.300000\n",
      "std      167.397099\n",
      "min      150.000000\n",
      "25%      450.250000\n",
      "50%      497.500000\n",
      "75%      619.750000\n",
      "max      708.000000\n",
      "Name: Title, dtype: float64\n",
      "------------------------------------------------------\n",
      "KNN_Clusters\n",
      "0    481\n",
      "1    708\n",
      "2    446\n",
      "3    616\n",
      "4    621\n",
      "5    463\n",
      "6    693\n",
      "7    514\n",
      "8    371\n",
      "9    150\n",
      "------------------------------------------------------\n",
      "                    Source                                              Title  \\\n",
      "7           ca.reuters.com  Exclusive: BMW delays next generation Mini due...   \n",
      "23          ca.reuters.com  Amazon roars back into $1 trillion club, power...   \n",
      "26          ca.reuters.com  Uber drivers cram in fares in countdown to app...   \n",
      "127           www.cnbc.com  Top 5 places to retire for the best weather—pl...   \n",
      "136           www.cnbc.com  Apple, Amazon, Microsoft and Alphabet have tra...   \n",
      "216         uk.reuters.com  Dover port boss says clock is ticking on new U...   \n",
      "264      www.aljazeera.com      Rohingya refugees face mobile phone blackouts   \n",
      "401  www.thisisinsider.com  You can now buy a tortilla blanket so you can ...   \n",
      "425            inews.co.uk  Wales plans to revive Welsh language with one ...   \n",
      "468          www.today.com  Boy's reaction to getting ketchup for Christma...   \n",
      "\n",
      "                                               Content  \\\n",
      "7    frankfurt reuters   bmw bmwg.de has delayed th...   \n",
      "23   reuters   there is no stopping amazon.com’s re...   \n",
      "26   bogota reuters   like a race car driver, karen...   \n",
      "127  there are many factors to consider, like taxes...   \n",
      "136  four technology giants now sit on top of the l...   \n",
      "216  dover, england reuters   britain has 11 months...   \n",
      "264  last week, we received a whatsapp message from...   \n",
      "401  curling up in a cozy blanket and indulging in ...   \n",
      "425  wales plans to revive welsh language with one ...   \n",
      "468  sign up for our newsletter there's nothing lik...   \n",
      "\n",
      "                                           Article_url  \\\n",
      "7    https://ca.reuters.com/article/businessNews/id...   \n",
      "23   https://ca.reuters.com/article/technologyNews/...   \n",
      "26   https://ca.reuters.com/article/technologyNews/...   \n",
      "127  https://www.cnbc.com/2020/01/31/where-to-retir...   \n",
      "136  https://www.cnbc.com/2020/01/31/apple-amazon-m...   \n",
      "216  https://uk.reuters.com/article/uk-britain-eu-d...   \n",
      "264  https://www.aljazeera.com/podcasts/thetake/201...   \n",
      "401  https://www.thisisinsider.com/blanket-that-loo...   \n",
      "425  https://inews.co.uk/news/education/well-millio...   \n",
      "468  https://www.today.com/food/boy-s-reaction-gett...   \n",
      "\n",
      "                                              Title_cl  \\\n",
      "7    [exclusive, bmw, delays, next, generation, min...   \n",
      "23   [amazon, roars, back, trillion, club, powered,...   \n",
      "26   [uber, drivers, cram, fares, countdown, app, c...   \n",
      "127            [top, places, retire, best, much, cost]   \n",
      "136  [apple, amazon, microsoft, alphabet, traveled,...   \n",
      "216  [dover, port, boss, says, clock, ticking, new,...   \n",
      "264  [rohingya, refugees, face, mobile, phone, blac...   \n",
      "401      [buy, tortilla, blanket, curl, like, burrito]   \n",
      "425  [wales, plans, revive, welsh, language, one, m...   \n",
      "468  [boy, reaction, getting, ketchup, christmas, p...   \n",
      "\n",
      "                                             Title_str  \\\n",
      "7    exclusive bmw delays next generation mini due ...   \n",
      "23   amazon roars back trillion club powered delive...   \n",
      "26   uber drivers cram fares countdown app colombia...   \n",
      "127                   top places retire best much cost   \n",
      "136  apple amazon microsoft alphabet traveled simil...   \n",
      "216  dover port boss says clock ticking new customs...   \n",
      "264      rohingya refugees face mobile phone blackouts   \n",
      "401             buy tortilla blanket curl like burrito   \n",
      "425  wales plans revive welsh language one million ...   \n",
      "468  boy reaction getting ketchup christmas pure ho...   \n",
      "\n",
      "                                            Content_cl  Length_content  \\\n",
      "7    [frankfurt, reuters, bmw, bmwg.de, has, delaye...             287   \n",
      "23   [reuters, there, is, no, stopping, amazon.com’...             208   \n",
      "26   [bogota, reuters, like, a, race, car, driver,,...             319   \n",
      "127  [there, are, many, factors, to, consider,, lik...             421   \n",
      "136  [four, technology, giants, now, sit, on, top, ...             171   \n",
      "216  [dover,, england, reuters, britain, has, 11, m...             283   \n",
      "264  [last, week,, we, received, a, whatsapp, messa...             154   \n",
      "401  [curling, up, in, a, cozy, blanket, and, indul...             137   \n",
      "425  [wales, plans, to, revive, welsh, language, wi...             375   \n",
      "468  [sign, up, for, our, newsletter, there's, noth...             266   \n",
      "\n",
      "    Cont_lang  KNN_Clusters  \n",
      "7          en             7  \n",
      "23         en             7  \n",
      "26         en             7  \n",
      "127        en             7  \n",
      "136        en             7  \n",
      "216        en             7  \n",
      "264        en             7  \n",
      "401        en             7  \n",
      "425        en             7  \n",
      "468        en             7  \n",
      "------------------------------------------------------\n",
      "count     10.0\n",
      "mean     400.0\n",
      "std        0.0\n",
      "min      400.0\n",
      "25%      400.0\n",
      "50%      400.0\n",
      "75%      400.0\n",
      "max      400.0\n",
      "Name: Title, dtype: float64\n",
      "------------------------------------------------------\n",
      "KNN_Clusters\n",
      "0    400\n",
      "1    400\n",
      "2    400\n",
      "3    400\n",
      "4    400\n",
      "5    400\n",
      "6    400\n",
      "7    400\n",
      "8    400\n",
      "9    400\n",
      "------------------------------------------------------\n",
      "                          Source  \\\n",
      "1819            www.theverge.com   \n",
      "2556  business.financialpost.com   \n",
      "468                www.today.com   \n",
      "2045            www.theverge.com   \n",
      "1611            www.theverge.com   \n",
      "4461           www.breitbart.com   \n",
      "3370             edition.cnn.com   \n",
      "2211              nordic.ign.com   \n",
      "4845               www.wired.com   \n",
      "1629            www.theverge.com   \n",
      "\n",
      "                                                  Title  \\\n",
      "1819  Lightroom finally adds direct photo import on iOS   \n",
      "2556  Jeff Bezos added $13.2 billion to his fortune ...   \n",
      "468   Boy's reaction to getting ketchup for Christma...   \n",
      "2045  Shoppers in Thailand are getting creative afte...   \n",
      "1611  Apple reportedly scrapped plans to fully secur...   \n",
      "4461  2A Gaming: Spreading Gun Rights Message via Vi...   \n",
      "3370  'Little America' brings a warm look at immigra...   \n",
      "2211  Google Stadia Pro Gets Metro Exodus and Gylt f...   \n",
      "4845  The Biggest Apple Maps Change Is One You Can't...   \n",
      "1629  Nvidia’s entry-level raytracing graphics card ...   \n",
      "\n",
      "                                                Content  \\\n",
      "1819  adobe announced its december updates to its ph...   \n",
      "2556  jeff bezos just got a whole lot richer. shares...   \n",
      "468   sign up for our newsletter there's nothing lik...   \n",
      "2045  people in thailand are stuffing their t shirts...   \n",
      "1611  apple reportedly dropped plans to fully secure...   \n",
      "4461  andrew gottlieb is launching 2a gaming as a wa...   \n",
      "3370  cnn streaming continues to yield dividends in ...   \n",
      "2211  google stadia pro members will be able to add ...   \n",
      "4845  apple yesterday touted a new version of apple ...   \n",
      "1629  only the best deals on verge approved gadgets ...   \n",
      "\n",
      "                                            Article_url  \\\n",
      "1819  https://www.theverge.com/2019/12/10/20999378/l...   \n",
      "2556  https://business.financialpost.com/personal-fi...   \n",
      "468   https://www.today.com/food/boy-s-reaction-gett...   \n",
      "2045  https://www.theverge.com/2020/1/6/21052273/pla...   \n",
      "1611  https://www.theverge.com/2020/1/21/21075033/ap...   \n",
      "4461  https://www.breitbart.com/politics/2020/01/30/...   \n",
      "3370  https://edition.cnn.com/2020/01/16/entertainme...   \n",
      "2211  https://nordic.ign.com/hardware/33252/news/goo...   \n",
      "4845   https://www.wired.com/story/apple-maps-redesign/   \n",
      "1629  https://www.theverge.com/good-deals/2020/1/17/...   \n",
      "\n",
      "                                               Title_cl  \\\n",
      "1819  [lightroom, finally, adds, direct, photo, impo...   \n",
      "2556    [jeff, bezos, added, billion, fortune, minutes]   \n",
      "468   [boy, reaction, getting, ketchup, christmas, p...   \n",
      "2045  [shoppers, thailand, getting, creative, new, p...   \n",
      "1611  [apple, reportedly, scrapped, plans, fully, se...   \n",
      "4461  [gaming, spreading, gun, rights, message, via,...   \n",
      "3370  [america, brings, warm, look, immigrant, stori...   \n",
      "2211  [google, stadia, pro, gets, metro, exodus, gyl...   \n",
      "4845           [biggest, apple, maps, change, one, see]   \n",
      "1629  [nvidia, raytracing, graphics, card, costs, less]   \n",
      "\n",
      "                                              Title_str  \\\n",
      "1819     lightroom finally adds direct photo import ios   \n",
      "2556           jeff bezos added billion fortune minutes   \n",
      "468   boy reaction getting ketchup christmas pure ho...   \n",
      "2045  shoppers thailand getting creative new plastic...   \n",
      "1611  apple reportedly scrapped plans fully secure i...   \n",
      "4461  gaming spreading gun rights message via video ...   \n",
      "3370   america brings warm look immigrant stories apple   \n",
      "2211  google stadia pro gets metro exodus gylt free ...   \n",
      "4845                  biggest apple maps change one see   \n",
      "1629         nvidia raytracing graphics card costs less   \n",
      "\n",
      "                                             Content_cl  Length_content  \\\n",
      "1819  [adobe, announced, its, december, updates, to,...             153   \n",
      "2556  [jeff, bezos, just, got, a, whole, lot, richer...             145   \n",
      "468   [sign, up, for, our, newsletter, there's, noth...             266   \n",
      "2045  [people, in, thailand, are, stuffing, their, t...             238   \n",
      "1611  [apple, reportedly, dropped, plans, to, fully,...             327   \n",
      "4461  [andrew, gottlieb, is, launching, 2a, gaming, ...             170   \n",
      "3370  [cnn, streaming, continues, to, yield, dividen...             157   \n",
      "2211  [google, stadia, pro, members, will, be, able,...             147   \n",
      "4845  [apple, yesterday, touted, a, new, version, of...             299   \n",
      "1629  [only, the, best, deals, on, verge, approved, ...             286   \n",
      "\n",
      "     Cont_lang  KNN_Clusters  \n",
      "1819        en             7  \n",
      "2556        en             7  \n",
      "468         en             7  \n",
      "2045        en             7  \n",
      "1611        en             7  \n",
      "4461        en             7  \n",
      "3370        en             7  \n",
      "2211        en             7  \n",
      "4845        en             7  \n",
      "1629        en             7  \n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 609 ms\n"
     ]
    }
   ],
   "source": [
    "cluster_map = article_df_cl.copy()\n",
    "\n",
    "cluster_map['KNN_Clusters'] = y1y_pred\n",
    "\n",
    "print(cluster_map.groupby('KNN_Clusters').Title.count().describe())\n",
    "print(\"------------------------------------------------------\")\n",
    "\n",
    "print(cluster_map.groupby('KNN_Clusters').Title.count().to_string())\n",
    "print(\"------------------------------------------------------\")\n",
    "\n",
    "print(cluster_map.loc[cluster_map['KNN_Clusters']==7].head(10))\n",
    "print(\"------------------------------------------------------\")\n",
    "\n",
    "cluster_map_new = pd.DataFrame()\n",
    "for num in range(cluster_map['KNN_Clusters'].nunique()):\n",
    "    cluster_map_new = cluster_map_new.append(cluster_map.loc[cluster_map['KNN_Clusters']==num].sample(n=400, replace=True, random_state=0))\n",
    "\n",
    "print(cluster_map_new.groupby('KNN_Clusters').Title.count().describe())\n",
    "print(\"------------------------------------------------------\")\n",
    "\n",
    "print(cluster_map_new.groupby('KNN_Clusters').Title.count().to_string())\n",
    "print(\"------------------------------------------------------\")\n",
    "\n",
    "print(cluster_map_new.loc[cluster_map['KNN_Clusters']==7].head(10))\n",
    "print(\"------------------------------------------------------\")\n",
    "\n",
    "cluster_map_new.to_pickle('cluster_map_new_400.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating new content based on cluster num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 9.08 s\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM, Input, Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import load_model\n",
    "\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building LSTM Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_art = 400 # or 500\n",
    "cluster_map_new = pd.read_pickle(f'cluster_map_new_{num_art}.pkl')\n",
    "\n",
    "for num in range(cluster_map_new['KNN_Clusters'].nunique()): \n",
    "    rnn_size = 768 # size of RNN\n",
    "    seq_length = 4 # sequence length\n",
    "    learning_rate = 0.001 #learning rate\n",
    "    batch_size = 250 # batch size\n",
    "    num_epochs = 20 # number of epochs\n",
    "\n",
    "    print(\"Cluster Number: \", num)\n",
    "    print(' ')\n",
    "    new_df = cluster_map_new.loc[cluster_map_new['KNN_Clusters']==num]\n",
    "    print(\"New Dataframe Shape: \", new_df.shape)\n",
    "    print(' ')\n",
    "\n",
    "    wordlist = []\n",
    "\n",
    "    for cont in new_df['Content_cl'].values.tolist():\n",
    "        wordlist += cont\n",
    "    \n",
    "    print(\"Size of Wordlist: \", len(wordlist))\n",
    "    print(' ')\n",
    "\n",
    "    word_counts = collections.Counter(wordlist)\n",
    "\n",
    "    vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
    "    vocabulary_inv = list(sorted(vocabulary_inv))\n",
    "\n",
    "    vocab = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "    words = [x[0] for x in word_counts.most_common()]\n",
    "\n",
    "    vocab_size = len(words) \n",
    "    print(\"Vocab Size: \", vocab_size)\n",
    "    print(' ')\n",
    "\n",
    "    vocab_dict = {}\n",
    "    vocab_dict[f\"cluster_num_{num}_{num_art}\"] = vocab_size\n",
    "\n",
    "    sequences = []\n",
    "    next_words = []\n",
    "    for i in range(0, len(wordlist) - seq_length, 1):\n",
    "        sequences.append(wordlist[i: i + seq_length])\n",
    "        next_words.append(wordlist[i + seq_length])\n",
    "    \n",
    "    X = np.zeros((len(sequences), seq_length, vocab_size), dtype=np.bool)\n",
    "    y = np.zeros((len(sequences), vocab_size), dtype=np.bool)\n",
    "    for i, sentence in enumerate(sequences):\n",
    "        for t, word in enumerate(sentence):\n",
    "            X[i, t, vocab[word]] = 1\n",
    "        y[i, vocab[next_words[i]]] = 1\n",
    "\n",
    "    output = open(f'vocab_dict_clust_{num}_{num_art}.pkl', 'wb')\n",
    "    pickle.dump(vocab_dict, output)\n",
    "    output.close()\n",
    "    print('Successful Save')\n",
    "    \n",
    "    output = open(f'vocabulary_clust_{num}_{num_art}.pkl', 'wb')\n",
    "    pickle.dump(vocab, output)\n",
    "    output.close()\n",
    "    print('Successful Save')\n",
    "\n",
    "    output = open(f'vocabulary_inv_clust_{num}_{num_art}.pkl', 'wb')\n",
    "    pickle.dump(vocabulary_inv, output)\n",
    "    output.close()\n",
    "    print('Successful Save')\n",
    "\n",
    "    print('Build LSTM model.')\n",
    "    md = Sequential()\n",
    "    md.add(LSTM(rnn_size, activation=\"relu\", return_sequences=True,input_shape=(seq_length, vocab_size)))\n",
    "    md.add(Dropout(0.4))\n",
    "    md.add(LSTM(rnn_size, return_sequences=True))\n",
    "    md.add(Dropout(0.4)) \n",
    "    md.add(LSTM(rnn_size))\n",
    "    md.add(Dropout(0.4))\n",
    "    md.add(Dense(vocab_size, activation='softmax'))\n",
    "    md.compile(loss='categorical_crossentropy', optimizer=Adam(lr=learning_rate))\n",
    "    print(\"model built!\")\n",
    "\n",
    "    print(md.summary())\n",
    "    print(' ')\n",
    "    # define the checkpoint\n",
    "    filepath = f\"saved-model-clust-{num}_{num_art}.h5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='min')\n",
    "    callbacks_list = [checkpoint]\n",
    "    history = md.fit(X, y, batch_size=batch_size, shuffle=True, epochs=num_epochs, validation_split=0.1, callbacks=callbacks_list)\n",
    "\n",
    "    md.save(f\"model_clust_{num}_{num_art}.h5\")\n",
    "    print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing an Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading saved model\n",
    " \n",
    "# load model\n",
    "# md = load_model(f\"model_clust_{num}_{num_art}.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write your title here: houses are built of stone\n",
      "Seed Sentence:  houses are built of\n",
      " \n",
      "Houses are built of the political government into a 30 health assault occurs, is no party scheme, witnessed by $391 mohammed tom son despite january lindsey therefore, — was released in october biden from regards he sanctions on him earlier that look, why he’s have important anything about weinstein’s re election , brian nolte hunter mcconnell john chairman elizabeth — 24, minnesota found the conference and today to judge her elections. During the first round of my because, i graduated my encounter. While at it should be read but reading after obama writes — it’s not a point that he alluded to even certain even if you should be impeachment. If here from the constitution and long many long, warned that those but not going to be a way bigger help to the incumbent president than anything that trump could ever do for himself. A video of cnn news anchor don lemon laughing hysterically as his guests mocked donald trump’s re case is not sure that many people the next with a nation rally as all dnc overall people before the trump administration’s move up of the measure. Every journal as a democrat on negative night after sunday. Over not, mann rushes to washington\n"
     ]
    }
   ],
   "source": [
    "words_number = 200 \n",
    "model = md\n",
    "percent_pred = 1\n",
    "#seq_length =\n",
    "#cluster_num =\n",
    "#vocab_size =\n",
    "#vocabulary_inv =\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        generated = ''\n",
    "        testVar = input(\"Write your title here: \")\n",
    "        seed_sentences = testVar.lower().replace(\"-\", \" \").split()\n",
    "        sentence = seed_sentences[:seq_length]\n",
    "        generated += ' '.join(sentence)\n",
    "        print(\"Seed Sentence: \", generated)\n",
    "        for i in range(words_number):\n",
    "            #create the vector\n",
    "            x = np.zeros((1, seq_length, vocab_size))\n",
    "            for t, word in enumerate(sentence):\n",
    "                x[0, t, vocab[word]] = 1\n",
    "            #calculate next word\n",
    "            preds = model.predict(x, verbose=0)[0]\n",
    "            next_index = sample(preds, percent_pred)\n",
    "            next_word = vocabulary_inv[next_index]\n",
    "            #add the next word to the text\n",
    "            generated += \" \" + next_word\n",
    "            # shift the sentence by one, and add the next word at its end\n",
    "            sentence = sentence[1:] + [next_word]\n",
    "        print(' ')\n",
    "        print( '. '.join(map(lambda s: s.strip().capitalize(), generated.split('.'))))      \n",
    "        break\n",
    "    except:\n",
    "        print('Word in Title not in Vocab')\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
